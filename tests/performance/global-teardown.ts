import { FullConfig } from '@playwright/test'
import fs from 'fs'
import path from 'path'

/**
 * Global teardown for performance tests
 * Processes results and generates performance reports
 */

interface PerformanceResults {
  testName: string
  metrics: {
    [key: string]: number
  }
  timestamp: string
  passed: boolean
}

async function globalTeardown(_config: FullConfig) {
  console.log('üìä Processing performance test results...')

  try {
    const resultsPath = path.resolve('./performance-test-results/results.json')

    if (!fs.existsSync(resultsPath)) {
      console.log('‚ö†Ô∏è  No performance results file found')
      return
    }

    // Read and process test results
    const resultsData = JSON.parse(fs.readFileSync(resultsPath, 'utf8'))
    const tests = resultsData.suites?.[0]?.specs || []

    const performanceResults: PerformanceResults[] = []

    // Extract performance metrics from test results
    tests.forEach((test: any) => {
      if (test.tests && test.tests.length > 0) {
        test.tests.forEach((testCase: any) => {
          const result: PerformanceResults = {
            testName: testCase.title,
            metrics: {},
            timestamp: new Date().toISOString(),
            passed: testCase.results?.[0]?.status === 'passed',
          }

          // Extract metrics from stdout/attachments if available
          const output = testCase.results?.[0]?.stdout || []
          output.forEach((line: any) => {
            if (typeof line === 'string' && line.includes('Performance Metrics:')) {
              try {
                // Parse performance metrics from console output
                const metricsMatch = line.match(/\{.*\}/)
                if (metricsMatch) {
                  const metrics = JSON.parse(metricsMatch[0])
                  result.metrics = metrics
                }
              } catch {
                // Failed to parse metrics
              }
            }
          })

          performanceResults.push(result)
        })
      }
    })

    // Save processed performance results
    const processedResultsPath = path.resolve('./performance-results-processed.json')
    fs.writeFileSync(processedResultsPath, JSON.stringify(performanceResults, null, 2))

    // Generate performance summary report
    generatePerformanceSummary(performanceResults)

    // Compare with baseline if available
    compareWithBaseline(performanceResults)

    console.log('‚úÖ Performance test results processed')
    console.log(`üìÑ Detailed results: ${processedResultsPath}`)
    console.log(`üìä Summary report: ./performance-summary.md`)
  } catch (error) {
    console.error('‚ùå Performance teardown failed:', error)
  }
}

function generatePerformanceSummary(results: PerformanceResults[]) {
  const passedTests = results.filter(r => r.passed).length
  const totalTests = results.length
  const passRate = totalTests > 0 ? Math.round((passedTests / totalTests) * 100) : 0

  const summary = `
# Performance Test Summary

**Generated:** ${new Date().toLocaleString()}
**Pass Rate:** ${passedTests}/${totalTests} (${passRate}%)

## Test Results

${results
  .map(
    result => `
### ${result.testName}
- **Status:** ${result.passed ? '‚úÖ PASSED' : '‚ùå FAILED'}
- **Timestamp:** ${result.timestamp}
- **Metrics:** ${
      Object.keys(result.metrics).length > 0
        ? Object.entries(result.metrics)
            .map(
              ([key, value]) => `${key}: ${typeof value === 'number' ? value.toFixed(2) : value}`
            )
            .join(', ')
        : 'No metrics available'
    }
`
  )
  .join('\n')}

## Performance Recommendations

${
  passRate < 100
    ? `
‚ö†Ô∏è  **Performance Issues Detected**
- ${totalTests - passedTests} test(s) failed performance thresholds
- Review failed test metrics above
- Consider optimizing slow-loading pages
- Check network conditions and bundle sizes
`
    : `
‚úÖ **All Performance Tests Passed**
- Application meets performance thresholds
- Continue monitoring for regressions
- Consider further optimizations for edge cases
`
}

## Monitoring Guidelines

1. **Run performance tests regularly** - Include in CI/CD pipeline
2. **Monitor Core Web Vitals** - Track LCP, FID, CLS metrics
3. **Optimize critical path** - Focus on above-fold content
4. **Bundle size monitoring** - Keep JavaScript bundles under 250KB initial
5. **Image optimization** - Use WebP format and lazy loading
6. **Network conditions** - Test on slow 3G connections

---
*Generated by Performance Test Suite*
`

  fs.writeFileSync('./performance-summary.md', summary)
}

function compareWithBaseline(results: PerformanceResults[]) {
  const baselinePath = path.resolve('./performance-baseline.json')

  if (!fs.existsSync(baselinePath)) {
    console.log('üìä No baseline found, current results will serve as baseline')
    return
  }

  try {
    const baseline = JSON.parse(fs.readFileSync(baselinePath, 'utf8'))
    const comparison: any = {
      timestamp: new Date().toISOString(),
      baseline: baseline.created,
      results: [],
    }

    results.forEach(result => {
      if (Object.keys(result.metrics).length > 0) {
        const testComparison = {
          testName: result.testName,
          current: result.metrics,
          baseline: baseline.measurements[result.testName] || {},
          changes: {} as any,
        }

        // Calculate percentage changes
        Object.entries(result.metrics).forEach(([metric, value]) => {
          const baselineValue = baseline.measurements[result.testName]?.[metric]
          if (baselineValue && typeof value === 'number') {
            const change = ((value - baselineValue) / baselineValue) * 100
            testComparison.changes[metric] = {
              change: change.toFixed(2) + '%',
              improved: change < 0,
              degraded: change > 10, // More than 10% slower is significant
            }
          }
        })

        comparison.results.push(testComparison)
      }
    })

    // Save comparison results
    fs.writeFileSync('./performance-comparison.json', JSON.stringify(comparison, null, 2))

    // Update baseline with current measurements
    baseline.measurements = {}
    results.forEach(result => {
      if (Object.keys(result.metrics).length > 0) {
        baseline.measurements[result.testName] = result.metrics
      }
    })
    baseline.lastUpdated = new Date().toISOString()

    fs.writeFileSync(baselinePath, JSON.stringify(baseline, null, 2))

    console.log('üìà Performance comparison completed')
    console.log('üìÑ Comparison results: ./performance-comparison.json')
  } catch (error) {
    console.warn('‚ö†Ô∏è  Failed to compare with baseline:', error.message)
  }
}

export default globalTeardown

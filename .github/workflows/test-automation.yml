name: Test Automation

on:
  push:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'tests/**'
      - '**.test.**'
      - '**.spec.**'
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/**'
      - 'tests/**'
      - '**.test.**'
      - '**.spec.**'
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - accessibility
          - visual

concurrency:
  group: test-automation-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Test matrix for different scenarios
  test-matrix:
    name: Test Matrix
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      fail-fast: false
      matrix:
        node-version: ['18', '20', '22']
        test-suite: ['unit', 'integration', 'component']
        include:
          - node-version: '20'
            test-suite: 'unit'
            coverage: true

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        if: matrix.test-suite == 'unit'
        run: |
          if [ "${{ matrix.coverage }}" == "true" ]; then
            npm run test:coverage
          else
            npm run test -- --reporter=verbose
          fi

      - name: Run integration tests
        if: matrix.test-suite == 'integration'
        run: |
          # Run tests that involve multiple components
          npm run test -- --testPathPattern=integration

      - name: Run component tests
        if: matrix.test-suite == 'component'
        run: |
          # Run focused component testing
          npm run test -- --testPathPattern=components

      - name: Upload coverage
        if: matrix.coverage == true
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage/lcov.info
          flags: unit-${{ matrix.node-version }}
          name: unit-tests-node-${{ matrix.node-version }}

  # Cross-browser E2E testing
  e2e-matrix:
    name: E2E Cross-Browser
    runs-on: ubuntu-latest
    timeout-minutes: 45

    strategy:
      fail-fast: false
      matrix:
        browser: ['chromium', 'firefox', 'webkit']
        shard: [1, 2, 3]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: Build application
        run: npm run build

      - name: Run E2E tests
        run: |
          npx playwright test \
            --project=${{ matrix.browser }} \
            --shard=${{ matrix.shard }}/3 \
            --reporter=html

      - name: Upload test results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-results-${{ matrix.browser }}-shard-${{ matrix.shard }}
          path: |
            test-results/
            playwright-report/
          retention-days: 7

  # Visual regression testing
  visual-testing:
    name: Visual Regression Tests
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright
        run: npx playwright install --with-deps chromium

      - name: Build application
        run: npm run build

      - name: Create visual test config
        run: |
          cat > playwright.visual.config.ts << 'EOF'
          import { defineConfig } from '@playwright/test';

          export default defineConfig({
            testDir: './tests/visual',
            use: {
              baseURL: 'http://localhost:4173',
              screenshot: 'only-on-failure',
            },
            projects: [
              {
                name: 'visual-chromium',
                use: { 
                  ...devices['Desktop Chrome'],
                  viewport: { width: 1280, height: 720 }
                },
              },
            ],
            webServer: {
              command: 'npm run preview',
              port: 4173,
              reuseExistingServer: !process.env.CI,
            },
          });
          EOF

      - name: Create visual tests
        run: |
          mkdir -p tests/visual

          cat > tests/visual/visual-regression.spec.ts << 'EOF'
          import { test, expect } from '@playwright/test';

          test.describe('Visual Regression Tests', () => {
            test('homepage visual test', async ({ page }) => {
              await page.goto('/');
              await page.waitForLoadState('networkidle');
              await expect(page).toHaveScreenshot('homepage.png');
            });
            
            test('login page visual test', async ({ page }) => {
              await page.goto('/login');
              await page.waitForLoadState('networkidle');
              await expect(page).toHaveScreenshot('login-page.png');
            });
            
            test('dashboard visual test', async ({ page }) => {
              // Mock authentication
              await page.goto('/login');
              await page.fill('[data-testid="email"]', 'test@example.com');
              await page.fill('[data-testid="password"]', 'password123');
              await page.click('[data-testid="login-button"]');
              
              await page.waitForURL('/dashboard');
              await page.waitForLoadState('networkidle');
              await expect(page).toHaveScreenshot('dashboard.png');
            });
            
            test('responsive design tests', async ({ page }) => {
              const viewports = [
                { width: 375, height: 667 },  // Mobile
                { width: 768, height: 1024 }, // Tablet
                { width: 1920, height: 1080 } // Desktop
              ];
              
              for (const viewport of viewports) {
                await page.setViewportSize(viewport);
                await page.goto('/');
                await page.waitForLoadState('networkidle');
                
                const suffix = viewport.width <= 375 ? 'mobile' : 
                              viewport.width <= 768 ? 'tablet' : 'desktop';
                await expect(page).toHaveScreenshot(`homepage-${suffix}.png`);
              }
            });
          });
          EOF

      - name: Run visual tests
        run: npx playwright test --config=playwright.visual.config.ts

      - name: Upload visual test results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: visual-test-results
          path: |
            test-results/
            tests/visual/
          retention-days: 7

  # Performance testing
  performance-testing:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright
        run: npx playwright install --with-deps chromium

      - name: Build application
        run: npm run build

      - name: Create performance test config
        run: |
          cat > playwright.perf.config.ts << 'EOF'
          import { defineConfig } from '@playwright/test';

          export default defineConfig({
            testDir: './tests/performance',
            use: {
              baseURL: 'http://localhost:4173',
            },
            projects: [
              {
                name: 'performance',
                use: { 
                  channel: 'chrome',
                  launchOptions: {
                    args: ['--enable-precise-memory-info']
                  }
                },
              },
            ],
            webServer: {
              command: 'npm run preview',
              port: 4173,
              reuseExistingServer: !process.env.CI,
            },
          });
          EOF

      - name: Create performance tests
        run: |
          mkdir -p tests/performance

          cat > tests/performance/performance.spec.ts << 'EOF'
          import { test, expect } from '@playwright/test';

          test.describe('Performance Tests', () => {
            test('page load performance', async ({ page }) => {
              const startTime = Date.now();
              
              await page.goto('/');
              await page.waitForLoadState('networkidle');
              
              const loadTime = Date.now() - startTime;
              console.log(`Page load time: ${loadTime}ms`);
              
              // Expect page to load within 3 seconds
              expect(loadTime).toBeLessThan(3000);
            });
            
            test('bundle size check', async ({ page }) => {
              const response = await page.goto('/');
              const transferSize = await response?.headerValue('content-length');
              
              if (transferSize) {
                const sizeKB = parseInt(transferSize) / 1024;
                console.log(`Initial bundle size: ${sizeKB.toFixed(2)}KB`);
                
                // Expect initial bundle to be under 500KB
                expect(sizeKB).toBeLessThan(500);
              }
            });
            
            test('memory usage', async ({ page }) => {
              await page.goto('/');
              await page.waitForLoadState('networkidle');
              
              const memoryUsage = await page.evaluate(() => {
                return (performance as any).memory?.usedJSHeapSize || 0;
              });
              
              if (memoryUsage > 0) {
                const memoryMB = memoryUsage / (1024 * 1024);
                console.log(`Memory usage: ${memoryMB.toFixed(2)}MB`);
                
                // Expect memory usage to be under 50MB
                expect(memoryMB).toBeLessThan(50);
              }
            });
            
            test('first contentful paint', async ({ page }) => {
              await page.goto('/');
              
              const fcp = await page.evaluate(() => {
                return new Promise((resolve) => {
                  new PerformanceObserver((list) => {
                    for (const entry of list.getEntries()) {
                      if (entry.name === 'first-contentful-paint') {
                        resolve(entry.startTime);
                      }
                    }
                  }).observe({ entryTypes: ['paint'] });
                });
              });
              
              console.log(`First Contentful Paint: ${fcp}ms`);
              expect(fcp).toBeLessThan(2000); // Under 2 seconds
            });
          });
          EOF

      - name: Run performance tests
        run: npx playwright test --config=playwright.perf.config.ts

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-test-results
          path: test-results/
          retention-days: 7

  # Load testing
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'performance'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Install artillery
        run: npm install --no-save artillery

      - name: Create load test config
        run: |
          cat > load-test.yml << 'EOF'
          config:
            target: 'http://localhost:4173'
            phases:
              - duration: 60
                arrivalRate: 5
                name: "Warm up"
              - duration: 120
                arrivalRate: 10
                name: "Sustained load"
              - duration: 60
                arrivalRate: 20
                name: "Peak load"

          scenarios:
            - name: "Homepage"
              weight: 40
              flow:
                - get:
                    url: "/"
                - think: 2
                    
            - name: "Login flow"
              weight: 30
              flow:
                - get:
                    url: "/login"
                - think: 3
                - post:
                    url: "/api/auth/login"
                    json:
                      email: "test@example.com"
                      password: "password123"
                      
            - name: "Dashboard"
              weight: 30
              flow:
                - get:
                    url: "/dashboard"
                - think: 5
          EOF

      - name: Start server
        run: |
          npm run preview &
          sleep 10

      - name: Run load tests
        run: |
          npx artillery run load-test.yml --output load-test-results.json

      - name: Generate load test report
        run: |
          npx artillery report load-test-results.json --output load-test-report.html

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: |
            load-test-results.json
            load-test-report.html
          retention-days: 7

  # Test reporting and aggregation
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test-matrix, e2e-matrix, visual-testing, performance-testing]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate test summary
        run: |
          echo "# üìä Test Automation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check job results
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.test-matrix.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests | ${{ needs.e2e-matrix.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Visual Tests | ${{ needs.visual-testing.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-testing.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Coverage" >> $GITHUB_STEP_SUMMARY
          echo "Coverage reports are available in the artifacts section." >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Next Steps" >> $GITHUB_STEP_SUMMARY
          if [[ "${{ needs.test-matrix.result }}" != "success" || \
                "${{ needs.e2e-matrix.result }}" != "success" || \
                "${{ needs.visual-testing.result }}" != "success" || \
                "${{ needs.performance-testing.result }}" != "success" ]]; then
            echo "‚ùå Some tests failed. Please review the failed jobs and fix issues before merging." >> $GITHUB_STEP_SUMMARY
          else
            echo "‚úÖ All tests passed! Ready for deployment." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const summary = `## üß™ Test Automation Results

            | Test Suite | Status |
            |------------|--------|
            | Unit Tests | ${{ needs.test-matrix.result == 'success' && '‚úÖ' || '‚ùå' }} |
            | E2E Tests | ${{ needs.e2e-matrix.result == 'success' && '‚úÖ' || '‚ùå' }} |
            | Visual Tests | ${{ needs.visual-testing.result == 'success' && '‚úÖ' || '‚ùå' }} |
            | Performance Tests | ${{ needs.performance-testing.result == 'success' && '‚úÖ' || '‚ùå' }} |

            ${context.payload.pull_request.draft ? 'üöß This is a draft PR' : ''}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
